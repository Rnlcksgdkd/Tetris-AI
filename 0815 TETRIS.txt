
## 끄적끄적 아이디어 란

- 현재 Epsi 에 따라 랜덤 행동으로 행동하도록 하는거 상당히 맘에 안듬
- A3C 방식 으로 Policy Gredient 설계방향?
- or Noise 방식으로 랜덤성을 부여하는 방향으로 부여하는 방향
- 몬테 카를로 방식 , 유전 알고리즘 참고 해보고 싶음
- 감가율은 고려하지 않아도 될까?
- 모듈 분리 : 학습기 , AI , 등등


## 학습 테스트

-  ver 6.4 테스트 : ver 6.3 에 이어서 학습 진행 ,  계속 오래 살아남도록 학습하는 것이 목표
	--> 생각보다 좋지 않은 결과.. 어떻게 하면 가장 이상적으로 쌓을 수 있지??
	


## 개발 사항

# 1. "콤보"를 학습하는 방법에 대한 생각 고려

- 기본적으로 경험하지 않는 메모리는 학습하지 못함..
- 4줄 콤보는 랜덤 행동으로는 거의 불가능 수준임
- 2줄 짜리는 확률적으로 낮긴한데 가능은 할듯?

- 직관적으로 애가 그런식으로 설계 할 수있도록 방향을 잡아줄 수 있을까?
- 추가적으로 좋은 결과에 대해서는 메모리에 우선순위를 두어 남기도록 설계? ::  우선순위 큐
- 메모리에 일부분을 고정영역으로 두어 좋은 행동을 메모리화 시켜 꾸준히 학습하도록 설계 해보기
- CNN 의 필터 숫자는 다시 늘리는게 좋지 않을까 싶다

# 2. 7가지 블록 타입에 대해 평평히 쌓기 도전

- 이전에는 상태영역이 너무 켜져서 힘들었던 것 같음
- 각 블록 타입 마다 불필요한 Action 줄여서 설계?
- 7가지 타입이 한꺼번에 학습이 힘들다면 4 / 3 으로 나눠서 파라미터 계승하는 방식으로 학습 

============ 고려  X=============
# 3. 게임 UI 및 대전 모드 개발

- 음.. 아직은 시기상조인듯

# 4. 대전테트리스에서 상대 에 대한 인풋 고려?

- 나중에 하기
==============================

## 개발 목표 플랜 

Ver 7 : 개발 자체는 별로 안걸릴듯?

- 학습 결과 시각화 기능 추가
- 각 블록 타입마다 불필요한 Action 제한하도록 아웃풋 세팅 
- Ver 7 은 4가지 블록 타입에 대해 콤보를 적용할 수 있는지에 대한 테스트가 목표 (2줄)
- CNN 필터 숫가 증가
- Limited Step 을 두어서 초반 빌드에 집중해보는 것도 좋은 방법이지 않나 싶음
- Memory 고정영역 두어 좋은 행동 꾸준히 학습하도록 세팅



- 